---
title: "The Story Behind Apple's 'Illusion of Thinking'"
date: "2025-6-18"
categories: ["Programing", "Performance", "LLMs"] 
tags: ["Architecture"]
---

## The Story Behind Apple's "Illusion of Thinking"

Apple just dropped a paper called *"The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity"* that takes some serious shots at Large Reasoning Models like GPT-4o, Claude, and Gemini. The basic argument? These models might look smart, but throw any real complexity at them and they crumble. According to Apple, this proves that emergent reasoning is either fragile or just plain fake.

But here's what's really going on: Apple is scrambling. They're behind on AI, their Apple Intelligence rollout has been a disaster, and now they're trying to convince everyone that being behind doesn't matter because the whole field is a dead end anyway. It's the tech equivalent of "I didn't want to go to that party anyway."

The timing of this paper is particularly telling. As AI researcher Henry Arithmaquine wrote on Twitter: "Be Apple, richest company in the world, every advantage imaginable. Go all in on AI, make countless promises. Get immediately lapped by anyone, two years into the race, nothing to show for it, give up, write a paper about how it's all fake and doesn't matter anyway."

## The Paper That Broke AI Twitter

When Apple released their paper in June 2025, it immediately went viral on AI Twitter, but not for the reasons Apple probably hoped. AI threader Ruben Hasid captured the mainstream interpretation: "Apple just proved AI reasoning models like Claude, DeepSeek R1, and 03 Mini don't actually reason at all. They just memorize patterns really well." His thread got 13.4 million views, though as he noted, "judging the way the likes fell off after these 13.4 million views, very few people made it past the first post."

The reaction from the AI community was swift and often brutal. Pliny the Liberator summed up many researchers' feelings: "I'm not reading a single AI research paper coming out of that giant stale donut in Cupertino until Siri can do a little bit more than create calendar events on the fourth try. If I were CEO of Apple and someone from my team put out a paper focused solely on documenting the limitations of current models, I'd fire everyone involved on the spot."

Andrew White of Future House SF pointed out this wasn't even Apple's first attempt at AI pessimism: "Apple's AI researchers have embraced a kind of anti-LLM cynic ethos, publishing multiple papers trying to argue that reasoning LLMs are somehow limited and cannot generalize. Apple also has the worst AI products."

## The Ceiling That Isn't Really There

Apple's researchers found that LRM accuracy drops to zero beyond a certain problem depth when tested on controlled puzzles like Tower of Hanoi, River Crossing, and Blocks World. They're treating this like it's some profound discovery about the limits of machine intelligence. But what they've actually found is that current models hit resource limits—token context, inference time, GPU memory.

The paper reveals something particularly interesting about how these models behave under pressure. As noted by Ars Technica's coverage of the research: "The researchers also identified what they call a 'counterintuitive scaling limit.' As problem complexity increases, simulated reasoning models initially generate more thinking tokens but then reduce their reasoning effort beyond a threshold, despite having adequate computational resources."

This behavior isn't evidence of fundamental limitations—it's evidence of intelligent resource management. As University of Toronto economist Kevin A. Bryan argued on X: "If you tell me to solve a problem that would take me an hour of pen and paper, but give me five minutes, I'll probably give you an approximate solution or a heuristic. This is exactly what foundation models with thinking are RL'd to do."

The models aren't giving up because they've reached some fundamental barrier to intelligence. They're giving up because they're running out of computational budget. When the cost-benefit analysis turns negative, they strategically retreat. It's like a student who stops trying on an exam when they realize they're running out of time—not because the problems are impossible, but because the constraints make success unlikely.

Long-time AI skeptic Gary Marcus, who called the Apple results "pretty devastating to LLMs," noted something particularly damning: "It is truly embarrassing that LLMs cannot reliably solve Hanoi... even when researchers provided explicit algorithms for solving Tower of Hanoi, model performance did not improve." Study co-lead Iman Mirzadeh argued this shows "their process is not logical and intelligent."

But this misses the point entirely. The fact that giving models explicit algorithms doesn't help suggests something much more interesting: these models aren't following algorithms in the traditional sense. They're doing something more complex and harder to interpret.

## Apple Intelligence: The Emperor's New AI

While Apple's researchers are busy explaining why real AI is impossible, their own AI products tell a different story. Mac O'Clock called Apple Intelligence "woefully underwhelming, badly implemented, technically poor." The review went on to describe it as "a victim of marketing" that fails to deliver on basic promises.

Reddit users are even harsher. One user on r/ios18beta wrote: "It's basically just an auto-thesaurus word swapper" while another complained: "It loses nuance... feels so useless, burning having upgraded hardware." The consensus seems to be that Apple Intelligence fails at the very tasks that make AI useful in the first place.

The really embarrassing part? The smartest features in Apple's AI suite aren't even Apple's. They quietly outsourced the heavy lifting to OpenAI's GPT-4o. So when Siri actually does something impressive, it's not Apple Intelligence at work—it's the competition. This creates an awkward situation where Apple is simultaneously criticizing AI reasoning capabilities while depending on competitors' AI for their own products' best features.

And Siri's big overhaul that was supposed to showcase Apple's AI prowess? Originally promised for 2024, then delayed to 2025, it's now not expected until spring 2026 according to multiple reports. That's not a minor setback—analysts say it's cost Apple around \$75 billion in market cap as investors lose confidence in the company's AI strategy.

## WWDC 2025: The Dud That Defined a Strategy

WWDC 2025 was supposed to be Apple's AI moment. Instead, we got what many called one of the most disappointing developer conferences in recent memory. The event focused heavily on visual changes like the "Liquid Glass" UI redesign, which left users squinting and confused rather than impressed.

Bloomberg's Mark Gurman, usually charitable toward Apple, tried to put a positive spin on things: "Excellent WWDC. Cohesive story, deep integration, and continuity across the devices. Zero false promises, impressive new UI, and significant new productivity features on the Mac and iPad." But even he had to admit: "The lack of any real new AI features, despite that being my expectation, is startling."

One WWDC attendee captured the mood in a widely-shared post: "Visual after visual in the presentation is worse than the previous. Apple needs to go back to its roots, make a really good operating system, make really good scaffolding for others to make the apps, and stuff that lives on the device. I'm completely underwhelmed. Apple needs a step change to their entire existence if things are going to turn around."

The absence of meaningful AI announcements at a time when the industry is rapidly advancing left many wondering if Apple has simply given up trying to compete. Instead of showcasing breakthrough AI capabilities, Apple seemed more focused on aesthetic changes that distract from their lack of substance.

## The Strategic Deflection Playbook

Apple's approach here follows a classic corporate damage control playbook. Step one: find weaknesses in your competitors' products. Step two: blow those weaknesses out of proportion and frame them as existential problems. Step three: rebrand your own shortcomings as "prudent caution" or "responsible AI development."

Investment community reactions show this strategy isn't working. Andrew Choi, a portfolio manager at Parnassus Investments, put it bluntly: "It's hard to argue that Apple's lack of standing with AI isn't an existential risk. If it can paint a future where it's integrating and commoditizing AI, that would be compelling, because otherwise, what is going to get people to buy their next phone for a lot more money?"

This gets to the heart of Apple's real problem. They've built a business model around convincing consumers to upgrade devices regularly, often for incremental improvements. But AI represents a potentially paradigm-shifting technology that could make hardware upgrades less important while making software capabilities more crucial. Apple's inability to lead in AI threatens not just their current products but their entire business model.

It's rhetorically clever but practically useless. You can't delay your own products and then declare victory by redefining success as never having to compete. The market keeps moving, and Apple increasingly looks like the company that canceled their moon shot and then wrote a paper about why the moon doesn't exist.

## What's Really Happening with Token Budgets

The models Apple tested learn to conserve resources when things get tough. When they start burning through their token budget without making progress, they cut their losses. This isn't a philosophical statement about the nature of thought—it's an engineering feature designed to prevent runaway costs.

As the Apple research team noted in their paper: "We observe that reasoning models initially increase their thinking tokens proportionally with problem complexity. However, upon approaching a critical threshold—which closely corresponds to their accuracy collapse point—models counterintuitively begin to reduce their reasoning effort despite increasing problem difficulty."

Apple is looking at this resource management behavior and concluding that the models have hit some deep conceptual wall. It's like watching someone turn off their car when it runs out of gas and concluding that cars are fundamentally incapable of transportation.

The paper reveals another fascinating pattern. As Ars Technica reported: "Claude 3.7 Sonnet could perform up to 100 correct moves in Tower of Hanoi but failed after just five moves in a river crossing puzzle—despite the latter requiring fewer total moves. This suggests the failures may be task-specific rather than purely computational."

This inconsistency isn't evidence of fundamental reasoning failure—it's evidence that these models work differently than we expect. They're not general-purpose reasoning engines; they're pattern-matching systems that have learned incredibly sophisticated patterns. Some patterns generalize better than others.

## The Emergence Apple Doesn't Want to See

The irony of Apple's position becomes clear when you look at the actual trajectory of AI development. GPT-3.5 couldn't do meaningful chain-of-thought reasoning. GPT-4 can. That's not an accident—it's emergence in action. We're seeing capabilities appear as models get bigger and better trained. It's messy, it's incremental, and it doesn't follow a neat linear progression. But it's real.

AI researcher community reactions on platforms like Hacker News highlight this disconnect. One commenter noted: "All Large 'Reasoning' Models do is generate data that they use as context to generate the final answer... They have no wit. They do not think or reason." But another responded: "This is like saying humans don't really think because we're just following neural patterns. The question isn't whether it's 'real' thinking—it's whether it produces useful results."

Apple is essentially arguing that because the process isn't perfect yet, it's fundamentally broken. That's like looking at the Wright brothers' first flight and concluding that powered flight is impossible because the plane only stayed up for 12 seconds.

The paper's own data actually supports the emergence thesis if you look closely. The researchers identified "three performance regimes": simple tasks where standard models outperform reasoning models, medium complexity tasks where reasoning models excel, and high complexity tasks where both fail. This isn't evidence against reasoning—it's evidence that reasoning models are specialized tools that work best in specific contexts.

## Hardware Limitations and the Real Story

One aspect that's gotten less attention but may be crucial is Apple's hardware situation. As Tom's Hardware reported, an expert "pours cold water on Apple's downbeat AI outlook—says lack of high-powered hardware could be to blame." The expert noted that Apple's hardware lacks the throughput for intense transformer runs, which could explain why Apple's internal testing shows such dramatic performance collapses.

This suggests that Apple's research may be inadvertently documenting their own infrastructure limitations rather than fundamental AI limitations. If you don't have the computational resources to run these models properly, of course they're going to fail on complex tasks.

The implications are significant. If Apple's negative results stem from hardware constraints rather than algorithmic limitations, then other companies with better infrastructure might not see the same performance cliffs. This would make Apple's paper less a universal critique of AI reasoning and more a documentation of their own competitive disadvantages.

## What Apple Should Actually Do

Instead of writing papers about why AI is doomed, Apple could focus on building foundation models that actually work locally without falling back on OpenAI. They could upgrade their infrastructure to handle serious transformer workloads. They could stop staging demos and start shipping features that work reliably in the real world.

Most importantly, they could treat token-budget behavior as useful data about system design rather than evidence that intelligence itself is an illusion. The fact that models learn to manage their computational resources intelligently should be seen as a feature, not a bug.

Apple could also invest in the kind of specialized hardware that would allow them to run these models without the performance collapses they documented. If the problem is computational resources rather than fundamental algorithmic limitations, then the solution is better hardware, not giving up on AI altogether.

But perhaps most importantly, Apple needs to develop a coherent AI strategy that goes beyond catching up to competitors. They need to identify what unique value they can provide in an AI-driven world, rather than simply arguing that AI doesn't matter because they can't do it well.

## Industry Context and Broader Implications

Apple's timing couldn't be worse from a market perspective. As they're publishing papers about AI limitations, competitors are achieving breakthrough results. DeepSeek's R1 model, which Apple criticizes in their paper, has shown remarkable capabilities at a fraction of the cost of other leading models.

The disconnect between Apple's research and market reality is becoming impossible to ignore. While Apple's researchers document failure modes, users are getting real value from AI reasoning models in everything from coding assistance to complex problem-solving.

This raises questions about whether Apple's research agenda is aligned with actual user needs or whether it's become too focused on justifying the company's strategic position. When your research consistently finds problems with technologies your competitors are successfully deploying, it might be time to examine whether the problem is with the technology or with your approach to it.

## The Real Illusion Here

Apple's *"Illusion of Thinking"* reads less like serious AI research and more like sophisticated damage control. The paper's conclusion that "reasoning traces are syntactic padding, not semantic computation" might be technically accurate, but it misses the larger point: whether or not it's "real" thinking (to which most who have studied human "thinking" will tell you we dont even know enough to know what we are even looking for yet), these systems are producing useful continually improving results that users value.

Emergent reasoning isn't bankrupt—it's challenging. It's a frontier worth exploring, not a dead end to avoid early. The fact that current implementations have limitations doesn't mean the approach is fundamentally flawed; it means there's more work to do.

Apple has every right to critique AI hype and point out current limitations. Rigorous evaluation of AI capabilities is valuable and necessary. But let's not confuse current engineering challenges (and one company's struggles with a piece of tech for that matter) with fundamental impossibilities. When and if Apple finally catches up, it won't be because they convinced everyone that intelligence is an illusion. It'll be because they once again decide to start building systems that actually work reliably.

Until then, their criticism feels a lot like projection: "We can't do this, so it must not be worth doing." The market, meanwhile, keeps moving forward, and users keep finding value in the very technologies Apple claims are illusory.

The real illusion here might be Apple's belief that they can wish away their competitive disadvantages by reframing them as industry-wide limitations. It comes off as egotistical and reeks of a disturbing internal culture surrounding the truth being whatever Apple says it is.
